from google.cloud import aiplatform
from google.cloud import storage # <--- NOVA IMPORTACI√ì
from google.api_core.exceptions import NotFound, Forbidden

# --- CONFIGURACI√ì ---
# PROJECT_ID = "aina-474214"   # <--- POSA EL TEU ID AQU√ç
PROJECT_ID = "aina-demostradors"   # <--- POSA EL TEU ID AQU√ç
REGION = "europe-west4"
# IMPORTANT: Els noms dels buckets s√≥n globals i √∫nics a tot Google Cloud.
# Et recomano usar: f"gs://{PROJECT_ID}-vertex-staging" per evitar conflictes.
STAGING_BUCKET = f"gs://{PROJECT_ID}-vertex-staging" 

# Noms per identificar els recursos
ENDPOINT_DISPLAY_NAME = "salamandra-7b-endpoint"
MODEL_DISPLAY_NAME = "salamandra-7b-instruct"
HF_MODEL_ID = "BSC-LT/salamandra-7b-instruct"

# Imatge Docker de Google per a vLLM
# Utilitzem una versi√≥ m√©s recent i estable (Desembre 2025) per corregir errors de versions anteriors
VLLM_DOCKER_URI = "us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20251211_0916_RC01_stable"

def garantizar_bucket(bucket_uri, project_id, location):
    """
    Comprova si el bucket existeix. Si no, el crea.
    """
    # Netejar el prefix 'gs://' si hi √©s, la llibreria storage vol nom√©s el nom
    bucket_name = bucket_uri.replace("gs://", "")
    
    storage_client = storage.Client(project=project_id)
    
    try:
        bucket = storage_client.get_bucket(bucket_name)
        print(f"‚úÖ Bucket existent trobat: {bucket_uri}")
    except NotFound:
        print(f"‚ö†Ô∏è El bucket {bucket_uri} no existeix. Creant-lo a {location}...")
        try:
            bucket = storage_client.create_bucket(bucket_name, location=location)
            print(f"‚úÖ Bucket creat correctament: {bucket_uri}")
        except Exception as e:
            print(f"‚ùå Error cr√≠tic creant el bucket: {e}")
            print("NOTA: Els noms de bucket han de ser √∫nics a tot el m√≥n.")
            raise e
    except Forbidden:
        print(f"‚ùå Error: El bucket existeix per√≤ no tens permisos per accedir-hi.")
        raise
    
    return bucket_uri

def get_or_deploy_salamandra():
    # 0. GARANTIR BUCKET (Pas previ)
    garantizar_bucket(STAGING_BUCKET, PROJECT_ID, REGION)

    # Ara ja podem inicialitzar Vertex AI amb seguretat
    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)

    # 1. COMPROVAR SI L'ENDPOINT JA EXISTEIX (Est√† corrent?)
    print(f"üîç Buscant endpoint existent: '{ENDPOINT_DISPLAY_NAME}'...")
    existing_endpoints = aiplatform.Endpoint.list(
        filter=f'display_name="{ENDPOINT_DISPLAY_NAME}"'
    )

    if existing_endpoints:
        endpoint = existing_endpoints[0]
        print(f"‚úÖ Endpoint trobat: {endpoint.resource_name}")
        print("   Connectant directament (sense espera)...")
        return endpoint

    print("‚ùå No s'ha trobat cap endpoint actiu.")

    # 2. COMPROVAR SI EL MODEL JA EST√Ä AL REGISTRE
    print(f"üîç Buscant model al registre: '{MODEL_DISPLAY_NAME}'...")
    existing_models = aiplatform.Model.list(
        filter=f'display_name="{MODEL_DISPLAY_NAME}"'
    )

    if existing_models:
        model = existing_models[0]
        print(f"‚úÖ Model trobat al registre: {model.resource_name}")
    else:
        print("‚ùå El model no est√† importat. Important des de Hugging Face...")
        # Importar model (triga uns 3-5 min)
        model = aiplatform.Model.upload(
            display_name=MODEL_DISPLAY_NAME,
            serving_container_image_uri=VLLM_DOCKER_URI,
            serving_container_command=["python", "-m", "vllm.entrypoints.api_server"],
            # serving_container_args=[
            #     f"--model={HF_MODEL_ID}",
            #     "--tensor-parallel-size=1",
            #     "--dtype=bfloat16",
            #     "--trust-remote-code"
            # ],
            serving_container_args=[
                f"--model={HF_MODEL_ID}",
                "--dtype=bfloat16",             # L4 soporta bfloat16 nativo (mejor precisi√≥n/velocidad)
                "--tensor-parallel-size=1",     # Correcto para 1 GPU
                "--gpu-memory-utilization=0.90", # CR√çTICO: Vertex necesita un margen de VRAM
                "--max-model-len=8192",         # RECOMENDADO: Limita el contexto para no saturar la L4
                "--trust-remote-code",          # Necesario para algunos modelos del BSC
                "--disable-log-stats"           # Opcional: Reduce el ruido en los logs de Google Cloud
            ],
            serving_container_ports=[8000],
            serving_container_predict_route="/generate",
            serving_container_health_route="/health",
        )
        print("‚úÖ Model importat correctament.")

    # 3. DESPLEGAR EL MODEL
    print(f"üöÄ Desplegant model a un nou Endpoint (aix√≤ trigar√† ~15-20 mins)...")
    # endpoint = model.deploy(
    #     endpoint_display_name=ENDPOINT_DISPLAY_NAME,
    #     machine_type="g2-standard-8",  # NVIDIA L4
    #     accelerator_type="NVIDIA_L4",
    #     accelerator_count=1,
    #     deploy_request_timeout=1800
    # )
    # PAS A: Crear l'Endpoint expl√≠citament amb el nom que volem
    endpoint = aiplatform.Endpoint.create(
        display_name=ENDPOINT_DISPLAY_NAME,
        project=PROJECT_ID,
        location=REGION
    )

    # PAS B: Desplegar el model dins d'aquest Endpoint
    model.deploy(
        endpoint=endpoint,  # Utilitzem l'endpoint que acabem de crear
        deployed_model_display_name=MODEL_DISPLAY_NAME,
        machine_type="g2-standard-8",  # NVIDIA L4
        accelerator_type="NVIDIA_L4",
        accelerator_count=1,
        deploy_request_timeout=1800
    )
    
    print(f"‚úÖ Desplegament completat: {endpoint.resource_name}")
    return endpoint

# --- EXECUTAR EL FLUX ---
if __name__ == "__main__":
    try:
        endpoint = get_or_deploy_salamandra()

        # --- PROVA DE TRADUCCI√ì ---
        print("\nüß™ Provant el model amb una pregunta en catal√†...")
        
        prompt = """<|im_start|>user
Com li explicaries a un nen de 5 anys qu√® √©s un ordinador? Respon en catal√†.<|im_end|>
<|im_start|>assistant"""

        response = endpoint.predict(instances=[{
            "prompt": prompt,
            "max_tokens": 300,
            "temperature": 0.7
        }])
        print("\nü§ñ Resposta de Salamandra:\n")
        print(response.predictions[0])
        
    except Exception as e:
        print(f"\n‚ùå S'ha produ√Øt un error: {e}")